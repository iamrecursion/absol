% How the problem is analysed to create the solution.
% Overall architecture of the design.
% Examine the design approaches taken.
% Identification of areas of the design that account for the requirements and resolve potential conflicts. 
% 

\chapter{Architecture and Algorithms} % (fold)
\label{cha:architecture_and_algorithms}
% This section will contain:
% \begin{itemize}
%     \item \textbf{Architectural Overview:} An overview of the system architecture, linked to the high-level requirements.
%     This will touch on the portions of the system that were ruled out of scope later on, and why.
%     \item \textbf{System Component Examination:} An examination of the design of each of the system components in detail.
%     It will also look at initial or discarded algorithmic designs as part of the process. 
% \end{itemize}

Following on from the design work put into the \gls{metaspec} (see \autoref{cha:designing_the_metalanguage}), significant time and effort was then invested into the design of the metacompiler toolchain itself, as well as the core algorithms and theory that it uses. 
This chapter aims to provide a high-level overview of the \gls{absol} toolchain, showing the main system components and linking these to the high-level requirements.
It also provides a firm background to the design and development of the algorithms utilised by the metacompiler itself, and develops the rigour behind the special-case semantics. \\

It should be noted that unlike \gls{metaspec}, which was designed over one consistent period, the design and implementation periods for the \gls{absol} toolchain were interleaved heavily.
This provided the opportunity to iterate on the designs where necessary.
Instances of this occurring will be noted throughout this chapter, but the designs presented below will be the final ones. 

\section{Designing the Metacompiler --- ABSOL} % (fold)
\label{sec:designing_the_metacompiler_absol}
As for any large system, it is important to be able to visualise the way in which the individual system components interact and are integrated. 
\gls{absol} itself is composed at a high-level of two main modules, Metaparse and Metaverify.
These modules form a natural segmentation of the work that the compiler has to do, and is are naturally subdivided internally.
\gls{absol} is best visualised as a pipeline, and the arrows illustrate the flow of data through the metacompiler.
The pipeline-style architecture is very suitable for \gls{absol}, as each stage of the toolchain depends only on the output of the previous stage.
The high-level architecture of the metacompiler is illustrated in \autoref{fig:absol_high_level_architectural_diagram} below.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{resources/images/metacompiler_pipeline_architecture.pdf}
    \caption{ABSOL High-Level Architectural Diagram}
    \label{fig:absol_high_level_architectural_diagram}
\end{figure}

The main components of the metacompiler can be described as follows:
\begin{itemize}
    \item \textbf{Metaparse:} Responsible for the generation of an \gls{ast} from the input metaspec file and verifying some precondition properties for the verification engine. 
    It has two main stages: lexing and parsing (which also performs precondition verification). 
    For further description of this stage please see \autoref{sub:metaparse_ast_generation}.
    \item \textbf{Metaverify:} Responsible for verifying the termination properties of the language.
    It consists of three main stages: \gls{ast} preprocessing, semantic inference and semantic verification.
    For further description of this stage please see \autoref{sub:metaverify_the_verification_engine}.
\end{itemize}

The following sections provide a detailed exploration of the design of these two main components, with a focus on the reasons behind the design choices that have been made.
It was known at design stage that the implementation language would be Haskell, so sporadic references are made to the language choice and how it may have impacted the system design.
Also below is an exploration of how the eventual intention for \gls{dsl} execution and type-checking have impacted the design of the metacompiler itself. 

\subsection{The Metacompiler Front-End} % (fold)
\label{sub:the_metacompiler_front_end}
The metacompiler front-end is the harness that wraps around both Metaparse and Metaverify to allow them to interact with the world. 
It is not truly part of the requirements for the toolchain, but would always be necessary to enable the core features to have real-world applicability. 
The front-end needed to handle the following features:
\begin{itemize}
    \item \textbf{Command-Line Option Parsing:} The metacompiler front-end needed to be capable of parsing options given to the tool on the command line.
    These options would be used to provide inputs to the metacompiler (such as the file to run on) and configure its behaviour. 
    \item \textbf{File Loading:} The front-end also needed to be capable of loading the input language specification.
    \item \textbf{Pipeline Coordination:} The final task for the front-end was to pass data between the pipeline stages.
\end{itemize}

Initial design for the metacompiler front-end focused on the identification of separate logical concerns for the front end component.
Given that its responsibilities are entirely distinct, this posed little issue.
As any target language, Haskell included, would provide facilities for file loading and the movement of data, the only component that required any significant design work was the command-line options. 

\subsubsection{Designing the Command-Line Options} % (fold)
\label{ssub:designing_the_command_line_options}
With Haskell's robust support for parsing libraries, including those dedicated for parsing command-line options, little consideration had to be given to the design of the \gls{cli} argument parser itself.
As a counterpoint, the options required careful consideration as they would impact the abilities of the user to control the system behaviour. 
The design of the command-line arguments took place before the scope reduction (as discussed in \autoref{sub:removed_design_elements}) took place.
Hence, there are references in this list to options that are not used in the final design.\\

Determining appropriate configuration options resulted from an examination of the responsibilities of the metacompiler and a consideration of which portions of its behaviour might benefit from being configurable.
The analysis was further informed by the behaviour of common build tools such as the generic \mintinline{text}{gmake} (GNU-Make) and the Haskell-specific \mintinline{text}{stack}.
The following list of options resulted:
\begin{itemize}
    \item \textbf{Input Filename:} The metacompiler operates on a language specification, and so needs to be provided with the input file.
    \item \textbf{Analysis Verbosity:} An oft-encountered configuration parameter for processes with output to the command-line, being able to control the verbosity of the language analysis and error reporting process seemed useful.
    As a result, the options design included a flag to enable full reporting.
    \item \textbf{Log File:} It seemed reasonable for users to want to output the analysis results to a log-file rather than do \mintinline{text}{stdout}.
    This configuration parameter was intended to do exactly that, allowing the user to easily store the results of analysis of their language.
    \item \textbf{Output Directory:} Provided to serve both the log file output and any eventual build artefacts, many build tools provide the ability to specify the target directory for any output files.
    This seemed important enough for the option to be included in this list. 
    \item \textbf{Language Name / Version:} As discussed in \autoref{sub:the_metadata_blocks}, \gls{metaspec} specifications contain language metadata. 
    It is conceivable, however, that \gls{dsl} authors may want to temporarily override these settings at build time.
    To this end, flags for providing alternative language names and language versions should be provided. 
    \item \textbf{Reporting:} As the metacompiler was originally intended to perform code-generation, it was conceivable that the users of the tool might want to verify the language they are developing without generating code from the specification. 
    Providing a flag to enable this would shorten the write-compile-debug cycle so common in development, and hence improve \gls{dsl} development workflow.
    \item \textbf{Cleanup:} Also mostly intended to interact with the code-generation stage, the cleanup flag was meant to delete all build artefacts resulting from code-generation.
    This is a common feature provided by build systems (e.g. \mintinline{sh}{make clean} or \mintinline{sh}{stack cleanup}, and hence seemed important to include.
\end{itemize}

% subsubsection designing_the_command_line_options (end)

% subsection the_metacompiler_front_end (end)

\subsection{Metaparse --- AST Generation} % (fold)
\label{sub:metaparse_ast_generation}
Metaparse is the subsystem of \gls{absol} that performs generation of an \gls{ast} from the input language specification.
This meant that it was responsible for providing both a \gls{lexer} and a \gls{parser} for \gls{metaspec}, as stated in Requirement~\reqref{req:ParseMetaspec}.
During the design process, however, it became clear that the parser was also the most appropriate place to perform some initial precondition checking required by the verification algorithms at later stages of the pipeline. 
The design of Metaparse was therefore heavily influenced both by its \textit{location} in the metacompiler pipeline and the \textit{data} which it was dealing with.
The goals for the design of Metaparse were hence set out as follows:
\begin{itemize}
    \item \textbf{Produce an AST from a Metaspec File:} The lexing and parsing processes had to result in an AST from the input file, or in the case where the input file was incorrect, produce a helpful syntax error. 
    \item \textbf{Verify the Preconditions for Metaverify:} As a parser, Metaparse will have had to traverse all portions of the \gls{ast}, allowing it to have all the information necessary to validate the verification preconditions.
\end{itemize}

\subsubsection{The Parser and Lexer Design} % (fold)
\label{ssub:the_parser_and_lexer_design}
Initial considerations for the design of Metaparse called for a two-stage design, with separate lexing and parsing stages. 
The original intention was for the lexer to operate directly on the character stream of the \gls{metaspec} file, producing a token stream which would then be provided to the parser for the creation of an \gls{ast}. 
This design represented a good separation of concerns, and provided an appropriate level of decoupling between the two processes. \\

However, the design of Metaparse was later informed quite significantly by the choice of implementation technology. 
Megaparsec, as discussed in \autoref{sub:megaparsec_improved_parsing}, provides a significantly improved set of utilities for creating lexing primitives for a given language. 
This meant, from a design standpoint, that merging the lexing and parsing steps would bring significant benefits to the system design:
\begin{itemize}
    \item \textbf{Efficiency:} As the \gls{ast}-generation process would now only have to make one pass over the character stream, performing one process of characters $\to$ tokens (contrasted with the dual passes for characters $\to$ tokens and then tokens $\to$ \gls{ast}), this means that the process as a whole is more efficient.
    \item \textbf{Simplicity:} Merging the two processes serves to simplify the design due to the removal of a (mostly unnecessary) boundary between lexing and parsing.
    This is 
    \item \textbf{Ease of Implementation:} This aforementioned simplicity will likely mean that the implementation of Metaparse is simplified.
    On a time-constrained project this is a big boon, and a big benefit of the integrated design.
\end{itemize}

While the merging of separate processes like this is often viewed as an improper separation of concerns, it is sometimes the case that certain technologies permit a novel approach that aids significantly in simplifying the design.
With Megaparsec, this is definitely the case as the lexing primitives it provides are \textit{intended} to be used in an integrated fashion to provide a fluid and easy to read parser. \\

Using Megaparsec for the parser also brings with it additional benefits for the design of the parser. 
Due to the provision of excellent parse-error reporting facilities, no design effort has to be dedicated to providing useful errors at parse time to the \gls{dsl} designer. 
Additionally, its nature as a parser combinator library means that the elements of the parser are clearly readable as expressions for the language grammar. 

% subsubsection the_parser_and_lexer_design (end)

\subsubsection{Designing the Metaparse AST} % (fold)
\label{ssub:designing_the_metaparse_ast}
Megaparsec operates on a very strongly-typed set of custom \gls{ast} data-types. 
This `tree' of types is one of the main components of the Metaparse design, and is created in accordance with the grammar for \gls{metaspec} that is outlined in \autoref{cha:designing_the_metalanguage}.
The process of creating this set of types is more a development task than a design one, as it is an almost direct translation of the \gls{metaspec} \gls{ebnf} grammar into Haskell's data types. \\

The key design element of this transformational process was to determine the level of abstraction at which it should take place:
\begin{itemize}
    \item \textbf{Representation of Productions:} Certain elements of the \gls{metaspec} grammar are not able to be directly represented in the form of Haskell data-types.
    As a result, the design process had to recognise where it was not possible to provide a direct representation and perform a transformation of the grammar while still remaining semantically correct.
    Due to the modular nature of the grammar, there were only a few instances where this was required to take place, an example of which can be seen below.
    \begin{minted}[xleftmargin=1cm, numbers=none]{text}
[ syntax-access-block | environment-access-rule ]
    \end{minted}
    This portion of the grammar exists within a more complex production, and representing the possibilities of this in Haskell would be difficult.
    Instead, it was factored out into a separate type representing this alternation, seen below
    \begin{minted}[xleftmargin=1cm, numbers=none]{text}
access-block-or-rule = 
    syntax-access-block | environment-access-rule ;
    \end{minted}
    \item \textbf{Indirections:} The structure of Haskell data-types (usually concerning type recursion) can preclude the representation of certain productions in \gls{ebnf}. 
    In such cases, it is often necessary to introduce an indirect type (equivalent to a production with a single node itself) to allow proper representation in the \gls{ast}. 
    An example of this indirection or `factoring out' can be seen below.
    The following production cannot be directly represented as Haskell data types:
    \begin{minted}[xleftmargin=1cm, numbers=none]{text}
semantic-rule = 
    environment-input-rule |
    environment-access-rule |
    special-syntax-rule |
    semantic-evaluation-rule-list;
    \end{minted}
    Some of the rule cases would be factored out as follows to ensure proper representation in Haskell while still retaining the correct grammatical structure.
    \begin{minted}[xleftmargin=1cm, numbers=none]{text}
semantic-rule = 
    environment-input-rule |
    environment-access-rule-proxy |
    special-syntax-rule-proxy |
    semantic-evaluation-rule-list;

environment-access-rule-proxy = environment-access-rule;
special-syntax-rule-proxy = special-syntax-rule;
    \end{minted}
\end{itemize}

These temporary alterations to the grammar in aid of designing the Metaparse \gls{ast} data-types were performed during implementation of the types themselves.
This means that the temporary alterations to the grammar were not retained in any consistent format, as they did not add any additional clarity to the \gls{ebnf} representation of \gls{metaspec}.
In most cases, they actually obscured the language grammar, which was undesirable.

% subsubsection designing_the_metaparse_ast (end)

\subsubsection{Precondition Verification in the Parser} % (fold)
\label{ssub:precondition_verification_in_the_parser}
The verification algorithms discussed in \autoref{sec:the_core_algorithms} are only correct in the presence of certain preconditions (Requirement~\reqref{req:VerifyLanguageConstruction}).
For more details on why these are required for verification correctness, please see \autoref{sub:verifier_precondition_validation} on \autopageref{sub:verifier_precondition_validation}.
These preconditions can briefly be described as follows:
\begin{itemize}
    \item All non-terminals that are \textit{used} must be \textit{defined} in scope (this may include non-terminals provided by language features).
    \item All non-terminals must be \textit{defined only once}.
    \item All used types must be \textit{in scope}.
    \item All used special-syntax must be \textit{in scope}. 
\end{itemize}

While these preconditions could easily be verified in a separate pass over the generated \gls{ast}, the parser itself has to touch every node in the AST as it generates the data structure.
This makes it an appropriate location to collect the necessary data to perform the verification.\\

The verification of these preconditions is an inherently stateful operation, and hence required some alterations to the initial design of the parser.
While, by default, a Megaparsec parser tracks some state (in order to provide appropriate parser error messages), this state is not accessible to the user. 
This means that the design must incorporate some additional state to track the data required to perform this verification.
It needs to record data as follows:
\begin{itemize}
    \item The non-terminals, types and special-syntax \textit{brought into scope} by the language imports.
    \item The non-terminals \textit{defined} by the language specification.
    \item The non-terminals \textit{used} by the language specification. 
\end{itemize}

With the parser capable of tracking this data, the verification algorithm is able to operate.
For details of the precondition verification algorithm itself, see \autoref{sub:verifier_precondition_validation} on \autopageref{sub:verifier_precondition_validation}. 

% subsubsection precondition_verification_in_the_parser (end)

\subsubsection{The Final Design for Metaparse} % (fold)
\label{ssub:the_final_design_for_metaparse}
% Establish the final design elements for the parser as a descriptive list.

Combining the results of the two-stage design process illustrated in the previous sections, the final design for Metaparse is as follows:
\begin{itemize}
    \item \textbf{Integrated Lexer and Parser:} The parsing process will use an integrated set of lexer primitives to allow a one-pass parse of the input \gls{metaspec} specification. 
    \item \textbf{Full AST Data-Type Tree:} The parser will operate over a comprehensive Haskell type-level representation of the \gls{metaspec} grammar. 
    This typed \gls{ast} will have undergone the necessary transformations so as to allow appropriate representation in Haskell while retaining grammatical correctness. 
    \item \textbf{Integration of Parser State:} The parser will have an integrated user state that is able to track all the data defined in \autoref{ssub:precondition_verification_in_the_parser}.
    This data will be tracked as the parse takes place. 
    \item \textbf{Utilisation of Parser State for Precondition Validation:}
    The parser will perform precondition validation on the tracked state using the algorithms described in \autoref{sub:verifier_precondition_validation}.
\end{itemize}

% subsubsection the_final_design_for_metaparse (end)

% subsection metaparse_ast_generation (end)

\subsection{Metaverify --- The Verification Engine} % (fold)
\label{sub:metaverify_the_verification_engine}
% Talk about the different modules, the verification preprocessor, the recursive nature of the algorithm for traversing the metaspec AST.
% Much like Metaparse, it did not make sense to separate the two kinds of verification that had to take place.
% Despite the fact that they are relatively disparate algorithms

Metaverify is the second of the main subsystems in \gls{absol} and it is intended to perform verification of the input language. 
Verification, in the case of the metacompiler, means determining whether the language is guaranteed to always terminate and, in cases where it does not, providing diagnostics as to why.
This component aims to satisfy Requirements~\reqref{req:VerifySemanticForm}, \reqref{req:VerifySemanticGuards} and \reqref{req:GenerationofVerificationReports}, and hence has the following goals:
\begin{itemize}
    \item \textbf{Verification of the Semantic Form:} Checking the form of all of the semantics defined for the language to ensure that they satisfy the necessary criteria for guaranteed termination. 
    \item \textbf{Verification of the Semantic Guards:} Ensuring totality of all defined semantics.
    This means that there will always be a semantics for any portion of \gls{dsl} code. 
    \item \textbf{Generation of Reports:} The generation of reports on the termination status of the language. 
    In cases where the language cannot be shown to always terminate, these reports contain information specifying exactly why it does not. 
\end{itemize}

The verification engine consists of a multi-stage process, that first performs a preprocessing step (see \autoref{ssub:the_verification_preprocessor}), and then traverses the resultant language structure to determine if the language terminates. 
The algorithms used to check termination are not discussed in this section, and can instead be found in \autoref{sec:the_core_algorithms}.

\subsubsection{The Verification Preprocessor} % (fold)
\label{ssub:the_verification_preprocessor}
While it contains all necessary data to perform verification, the \gls{ast} that is generated by Metaparse is not the most efficient structure to verify the language. 
Simply put, this is because the verification algorithm requires some summary data, while other portions of the AST are just irrelevant.
To rectify this, the need for a \textit{preprocessing step} was identified at the design stage. \\

The role of the preprocessor is to take the full \gls{ast} that is output by Metaparse and extract the relevant data from it. 
At a high level this means it is an algorithm (with no dependence on state) with inputs and outputs as follows:
\begin{itemize}
    \item \textbf{Input(s):} The \gls{ast} for the input language $L$.
    \item \textbf{Output(s):} The truths defined for $L$, the start rule for $L$ and the additional language productions (non-terminals) defined for $L$. 
\end{itemize}

Due to the fact that the methods for traversing an \gls{ast} data-structure are inherently tied to the representation of the structure, it is somewhat difficult to represent this algorithm in a general format.
The algorithm is also responsible for collating these outputs in such a fashion that they are associated with the required tracking data for their termination state (for the contents of this data see \autoref{ssub:reporting_structures}).
This tracking data is known to the project as a \textit{tag}, and these are described in more detail later. 
The best algorithmic representation can thus be seen in \autoref{alg:the_verification_preprocessor_algorithm}, which assumes the existence of a number of functions as follows: 
\begin{itemize}
    \item \textbf{findStartRule} --- This function finds the start rule in the AST and extracts it, before associating it with a tag.
    \item \textbf{defaultTag} --- generates a default tag for the rule (see \autoref{ssub:reporting_structures} for more information).
    \item \textbf{extractTruths} --- Finds the defined truths for the languages, and extracts a list of the associated non-terminal symbols.
    These symbols are those known to terminate.
    \item \textbf{extractProductions} --- This function finds all productions (non-terminals) defined by the language and extracts them as pairs (Non-Terminal, Production Body). 
\end{itemize}

\begin{algorithm}
\begin{algorithmic}
\Require{The language AST is available as input}
\Ensure{The collated output matches the relevant portions of the AST}
\State
\State ast $\gets$ input
\State
\State $P_s \gets$ \textbf{findStartRule} 
\State startRule $\gets$ (\textbf{defaultTag}, $P_s$)
\State $T \gets$ \textbf{extractTruths}
\State $P_n \gets$ \textbf{extractProductions}
\State $P \gets []$
\Comment The empty list
\ForAll{$(N, P_i) \in P_n$}
\Comment Transform productions into searchable form
    \State $P \gets (N, (\textbf{defaultTag}, P_i)) : P$
    \Comment where $:$ is the list cons operator
\EndFor
\State
\State return (startRule, $T$, $P$)
\end{algorithmic}
\caption{The Verification Preprocessor Algorithm}
\label{alg:the_verification_preprocessor_algorithm}
\end{algorithm}

% subsubsection the_verification_preprocessor (end)

\subsubsection{Designing the Verification Engine} % (fold)
\label{ssub:designing_the_verification_engine}
% Need to talk about the stateful nature of the algorithm. 
% How both kinds of verification were best combined. 

While the verification engine itself is algorithmically dependent on the theory explored in \autoref{sec:the_core_algorithms}, this theory alone does not make a design. 
From an examination of these algorithms two main things are immediately apparent:
\begin{itemize}
    \item \textbf{Stateful Nature:} The algorithms operate in a stateful fashion.
    This is because they track the current state of the verification engine at any given point.
    \item \textbf{Integrated Traversal:} While there are two disparate semantic verification algorithms --- one dealing with semantic inference (\autoref{sub:semantic_inference}) and the other verifying defined semantics (\autoref{sub:basic_semantic_form_verification}) --- they both utilise the same traversal mechanism for the verification data. 
    This implies that both kinds of verification should be done in one pass, and that is indeed the form of the algorithm that performs the traversal in \autoref{sub:verifier_traversal}. 
\end{itemize}

Given that the bulk of this system component involves concrete implementations of the verification algorithms, the core design problem thus revolves around the state. 
The state for this algorithm provides three main functions:
\begin{itemize}
    \item \textbf{Tracking Verification Status:} The state must contain the current state of the verification for the language, including the tags for each production in the language.
    \item \textbf{Containing Algorithmic Resources:} The state must contain the information required by the verification algorithm.
    This predominantly means the outputs of the preprocessor algorithm as discussed in \autoref{ssub:the_verification_preprocessor}.
    \item \textbf{Tracking Temporary State:} As the algorithm progresses, certain portions of it need some awareness of where they have been.
    This helps to prevent mutual recursion between productions and other such non-terminating behaviour. 
\end{itemize}

% subsubsection designing_the_verification_engine (end)

\subsubsection{Reporting Structures} % (fold)
\label{ssub:reporting_structures}
% The RuleTag concept should be discussed here. 
% But the expansion of it: how it is designed in practice. 

While the generation of the reports themselves (Requirement~\reqref{req:GenerationofVerificationReports}) is nothing but an exercise in pretty-printing (and thus required no real design work), the recursive nature of the algorithm posed some difficulty for collating this data in the first place. 
Tracking the termination states, as defined in the verification algorithm (\autoref{ssub:termination_states}), is one of the more interesting design challenges for the Metaverify component.\\

From an examination of the core algorithm of the verifier (\autoref{sec:the_core_algorithms}), it is clear that the Metaverify component requires some practical method of tracking the termination state of each rule. 
However, this is not as simple as it might appear at first glance: in the cases where the rule is defined as non-terminating, the tag must also track the associated diagnostic data. \\

This is further complicated by the fact that the verification algorithms might involve arbitrary degrees of recursion. 
Consider, for example, a production \mintinline{text}{<p> ::= <a> | <b> | <c>}, where the algorithm recurses on each of the non-terminals in the alternation. 
As each of these may have a different termination result, the result for \mintinline{text}{<p>} is the result of combining all the sub-results. 
The termination result combination is specified in \autoref{ssub:termination_states}, so the design problem here is how to retain the correct non-termination data. 
In the case of a non-termination, the following data should be retained:
\begin{itemize}
    \item The type of the non-termination.
    \item An error describing the exact nature of the non-termination.
    \item A trace of where the non-termination occurred in relation to the rule being tagged. 
\end{itemize}

At each combination point, therefore, two things need to happen:
\begin{enumerate}
    \item All of the non-termination reasons need to be stored.
    \item The non-termination trace needs to be updated for \textit{all} of the stored reasons. 
\end{enumerate}

% subsubsection reporting_structures (end)

\subsubsection{Finalising the Metaverify Design} % (fold)
\label{ssub:finalising_the_metaverify_design}
From the above design work, the final design for the Metaverify system component can be understood as follows:
\begin{itemize}
    \item \textbf{Verification Preprocessor:} The component must implement a preprocessor to transform the \gls{ast} into an appropriate form for verification.
    \item \textbf{Verification State:} Metaverify must provide a mechanism for tracking the state that the algorithm requires to operate (see \autoref{ssub:designing_the_verification_engine}).
    \item \textbf{Reporting Structures:} The verification engine must be able to accurately track the termination state (and associated metadata) of all of the portions of the language that it is verifying. 
    This is done via a reporting structure as discussed in \autoref{ssub:reporting_structures}.
\end{itemize}

% subsubsection finalising_the_metaverify_design (end)

% subsection metaverify_the_verification_engine (end)

\subsection{Type-Checking} % (fold)
\label{sub:type_checking}
As part of the process of designing \gls{absol}, significant consideration was given to performing type-checking at the language level. 
It emerged fairly quickly, however, that this would not be possible. \\

This unfortunate circumstance actually arises from one of the main points of flexibility designed into the typing discipline of the semantics. 
As \gls{metaspec} provides a number of generic functions (those for which the types involved are not known at language design time), it is impossible to typecheck the language at this stage.
This impossibility is concisely embodied in the \mintinline{text}{any} type (part of the \mintinline{text}{base} module), that acts as an effective type `hole'.
The presence of \mintinline{text}{any} as a type in the expression means that the type cannot be determined at the language level, as it can only be filled when types are instantiated at program compile time.
As a result, the process of type-checking the DSL programs is deferred until program compilation time, rather than performed at the language verification stage.\\

There was some initial worry during this process that not being able to verify the types would compromise the correctness of the language verification, but thankfully this is not the case:
\begin{itemize}
    \item The language checker ensures that any expression in the \gls{dsl} that can be parsed has associated semantics.
    \item It also guarantees that these semantics will always terminate.
    \item At compile-time, the \gls{dsl} compiler is able to reject the input program if the types for all different instantiations of the \mintinline{text}{any} type in the program do not resolve correctly. 
\end{itemize}

As a result, it is clear that not being able to typecheck at the level of the language semantics will not have an impact on the correctness of the verification mechanism.

% subsection type_checking (end)

\subsection{Removed Design Elements} % (fold)
\label{sub:removed_design_elements}
The design presented at the start of this section in \autoref{fig:absol_high_level_architectural_diagram} is not the original design for the system.
Towards the start of the design phase for \gls{absol}, the scope of the project was much more grand. 
In addition to the areas in the final design, the system was also originally intended to encompass the following:
\begin{itemize}
    \item \textbf{DSL Compiler Generation:} The original scope of the metacompiler called for the generation of a DSL compiler from the language specification provided by \gls{metaspec}. 
    While this was an important part of making the toolchain truly useful for designing and implementing \glspl{dsl}, it is far from novel.
    The code-generation stage would have been heavily based upon work explored in \autoref{sec:automating_the_generation_of_the_compiler}, and would require significant design and implementation effort, likely exceeding the amount of time available to the project.
    \item \textbf{Compilation of DSL Programs:} The generated code for the DSL compiler would be intended to be compiled itself. 
    The resultant build artefact would be capable of transpiling programs in the target DSL to Haskell.
    These haskell programs would be prepared for interaction via the C \gls{ffi} with any host language that would want to interact with the DSL.
\end{itemize}

Unfortunately, due to scoping concerns, these additional parts of the metacompiler toolchain were ruled as `out of scope' for the project. 
While disappointing, care was taken to ensure that the novel parts of the project (the metalanguage and language verification capabilities) remained in scope as part of the core toolchain. 

% subsection removed_design_elements (end)

% section designing_the_metacompiler_absol (end)

\section{The Core Algorithms} % (fold)
\label{sec:the_core_algorithms}
% Talk about the design of each of the core algorithms in detail.
% The multiple-function deep mutual recursion. 
At the core of the design of \gls{absol} are the verification algorithms. 
Being so crucial to the functionality and success of the project, these underwent significant design work in isolation, and evolved over time during the implementation.
Much as for the design of the system components themselves, the algorithmic designs evolved over time, and so this section aims to present the final algorithms as well as discuss any changes that occurred over time. \\

The core algorithms can be separated into two main parts.
The first is the precondition validation algorithm that takes place at parse time (see \autoref{ssub:precondition_verification_in_the_parser}).
This ensures that the \gls{ast} resultant from parsing the language does not contain certain states permitted by the grammar but not by the verification algorithms preconditions. 
The second is the validation algorithm itself.
This is separated into the inference and validation sections, the latter of which is further subdivided into validation for the different kinds of semantic rules encountered. \\

The verification algorithms aim to verify the language against two kinds of non-totality:
\begin{enumerate}
    \item \textbf{Divergent Programs:} Programs \gls{diverge} due to infinite loops or non-terminating recursion.
    \item \textbf{Undefined Programs:} Programs that, for some language expressions, do not have defined semantics or have indeterminate semantics.
\end{enumerate}

The key recognition about this verification process is that it is \textit{enabled} only by the restricted form of the semantics that can be represented in \gls{metaspec}.
In general, this kind of verification is an instance of the \gls{halting_problem}, and hence impossible.
It is through placing stringent restrictions on the form of the semantics for these \glspl{dsl} that it is possible to make this process work. 

\subsection{Verifier Precondition Validation} % (fold)
\label{sub:verifier_precondition_validation}
For the verification algorithm to produce correct results, there are a set of preconditions that must hold. 
These preconditions are assumed to hold by the algorithms used for verification, and so they must be ensured to hold in order to guarantee that the verification of the language is correct.
These preconditions are as follows, where scope is defined in all cases as in the first precondition:
\begin{itemize}
    \item \label{item:non_terminal_usage} \textbf{Non-Terminal Usage:} Any non-terminal \textit{used} on the right-hand-side of a rule definition must be \textit{defined} in the current scope. 
    The non-terminals in the current scope are those defined by the \gls{dsl} language specification, and those imported by the language features specified in the \mintinline{text}{using} definition block (\autoref{sub:the_imports_definition_block}).
    If this precondition was not satisfied, it would be possible to have productions whose sub-terms do not have defined semantics and thus that the language would not be total. 
    \item \textbf{Single Definition:} Any non-terminal of a given name must be \textit{defined only once} in the language scope.
    Were this precondition not satisfied, it would be possible to have indeterminate semantics for a given program as the choice of production is undefined.
    \item \textbf{Types in Scope:} All types used by the language definition must be \textit{in scope}.
    As types have associated semantics at \gls{dsl} compile-time, having types not in scope could lead to the generation of incorrect programs. 
    This condition, however, does not lead to incorrectness at the language level, as all types have defined semantics.
    \item \textbf{Special-Syntax in Scope:} All special syntactic forms used by the language definition must be in scope.
    As special syntactic forms provide language semantics, not having the syntax in scope when it is used would lead to undefined semantics for programs in the \gls{dsl}. 
\end{itemize}

Verifying that these preconditions hold is, as mentioned previously, performed as part of Metaparse (\autoref{ssub:precondition_verification_in_the_parser}).
The verification is a stateful process, and hence the parser is required to track the following information:

\subsubsection{The Precondition Checking Algorithm} % (fold)
\label{ssub:the_precondition_checking_algorithm}
The algorithm to check that the preconditions hold is fairly simple, and operates in a three stage process.
The first stage of the process involves gathering the required information at the start: finding the imported language features and determining which non-terminals, types and special syntax elements they define.
The second stage of the process operates during the parse, and checks every type and piece of special syntax for being in scope. 
At the same time, any non-terminal being defined is checked against a list of already-defined non-terminals and any non-terminal being used is stored.
The final stage of the process operates on the lists of defined and used non-terminals, checking that all members of the latter are members of the former.\\

The algorithm for precondition verification is illustrated in \autoref{alg:the_precondition_verification_algorithm}, and assumes the existence of the following functions:
\begin{itemize}
    \item \textbf{importedNTs:} This function obtains the set of non-terminals defined by the imported language features.
    \item \textbf{importedTypes:} This function obtains the list of imported types.
    \item \textbf{importedSpecialSyntax:} This function obtains the list of special-syntax keywords imported by the language features. 
    \item \textbf{isNT/isType/isSpecial:} Determines whether the current AST node is a non-terminal, type or special-syntax keyword respectively.
    \item \textbf{defining:} Determines whether the current non-terminal being parsed is being defined or used. 
    \item \textbf{error:} Generates an error message.
\end{itemize}

This algorithm is sufficient to check that the preconditions for verification hold, and \textit{must} be executed before the verification algorithm itself.

\begin{algorithm}
\begin{algorithmic}
\Require{The Language AST is Present}
\Ensure{The preconditions are validated}
\State
\State definedNTs $\gets$ \textbf{importedNTs}()
\Comment A set.
\State usedNTs $\gets$ []
\Comment [] is the empty list
\State types $\gets$ \textbf{importedTypes}()
\Comment A list.
\State syntax $\gets$ \textbf{importedSpecialSyntax}()
\Comment A list.
\State
\ForAll{$N_i$ $\in$ language}
\Comment $N_i$ is a node in the AST
    \State \Call{checkNode}{$N_i$, definedNTs, usedNTs, types, syntax}
\EndFor
\State
\ForAll{NT $\in$ usedNTs}
    \If{NT $\notin$ definedNTs}
        \State \textbf{error}(NT used but not defined)
    \EndIf
\EndFor
\State
\Function{checkNode}{$N_t$, definedNTs, usedNTs, types, syntax}
    \If{\textbf{isNT}($N_t$)}
        \If{\textbf{defining}($N_t$)}
            \If{$N_t \in$ definedNTs}
                \State \textbf{error}($N_t$ already defined)
            \Else
                \State definedNTs $\gets N_t +$ definedNTs
                \Comment $+$ is the set addition operator
            \EndIf
        \Else
            \State usedNTs $\gets N_t : $ usedNTs
            \Comment $:$ is the list cons operation
        \EndIf
    \ElsIf{\textbf{isType}($N_t$) $\land$ $N_t \notin$ types}
        \State \textbf{error}($N_t$ not a defined type)
    \ElsIf{\textbf{isSpecial}($N_t$) $\land$ $N_t \notin$ syntax}
        \State \textbf{error}($N_t$ not valid syntax)
    \Else
        \State continue
    \EndIf
\EndFunction
\end{algorithmic}
\caption{The Precondition Verification Algorithm}
\label{alg:the_precondition_verification_algorithm}
\end{algorithm}

% subsubsection the_precondition_checking_algorithm (end)

% subsection verifier_precondition_validation (end)

\subsection{Verifier Traversal} % (fold)
\label{sub:verifier_traversal}
% The process of traversing the preprocessed AST and the reasons for it. 
% This section should contain details of the primary algorithm of the verifier. 
% Needs to talk about the termination states and how they combine from a theoretical standpoint. 
% want to write the algorihm recursively, so need to define a function.
% Make sure to establish the inputs as preconditions, and that it is available as state (however this may be). 
% TODO write it in terms of the concepts of a map(k,v)

The aim of the verification algorithm for \gls{absol} is to ensure that the semantics provided by the \gls{dsl} designer match the set of restrictions imposed to allow the termination proof. 
The exact nature of these restrictions varies based on the kind of semantics being considered (of the four available in \gls{metaspec} --- see \autoref{lst:language_rule_semantics}), but in all cases it is the job of this algorithm to ensure that the conditions hold for the language to be total. \\

The algorithm as a whole operates on a preprocessed version of the \gls{ast}, collated as discussed in \autoref{ssub:the_verification_preprocessor}. 
This preprocessor provides the following information to the algorithm:
\begin{itemize}
    \item \textbf{Truths:} The truths defined by the language designer.
    These are the base-cases for the main termination proof mechanism, and are assumed by this algorithm to always terminate.
    \item \textbf{Start Rule:} The body of the start rule for the language and a tag associated with it.
    This tag tracks the overall termination state of the language.
    \item \textbf{Non-Terminals:} A mapping from each non-terminal for which there was a user-supplied production to a pair of (tag, rule body).
    The tag is used to track the termination state of each non-terminal. 
\end{itemize}

\subsubsection{Termination States} % (fold)
\label{ssub:termination_states}
As part of this algorithm, each non-terminal in the language being analysed is marked with a given state.
In the project, these termination states are referred to as the \textit{tag} for the rule.
These tags may take one of four values:
\begin{itemize}
    \item \textbf{Untouched:} This is the initial state for all tags, and indicates that the entity associated with the rule (either a non-terminal or the start rule) has not yet been visited by the algorithm.
    \item \textbf{Touched:} The entity has been visited by the algorithm, but the algorithm is so far unable to assign it a termination state.
    \item \textbf{Does Not Terminate:} The algorithm has determined that the entity associated with this tag does not terminate. 
    \item \textbf{Terminates:} The algorithm has determined that the entity associated with this tag does not terminate.
\end{itemize}

As it is often the case that the tag result for a given production is the combination of the tag results for the non-terminals in the associated rule body, some thought was given to how best to combine these tag results.
The most intuitive way to define this combination was as a binary operation on values of the tag. 
After defining the operation, it became apparent that it was both associative and that an identity element existed, and hence the tag type is a \gls{monoid}.
The associative operation, `tagPlus' is defined as follows (using Haskell notation for convenience, the \mintinline{haskell}{_} refers to any value):
\begin{minted}[numbers=none]{haskell}
tagPlus :: Tag -> Tag -> Tag
tagPlus Untouched _                         = Untouched
tagPlus _ Untouched                         = Untouched
tagPlus DoesNotTerminate DoesNotTerminate   = DoesNotTerminate
tagPlus DoesNotTerminate _                  = DoesNotTerminate
tagPlus _ DoesNotTerminate                  = DoesNotTerminate
tagPlus Terminates Terminates               = Terminates
tagPlus Touched x                           = x
tagPlus x Touched                           = x
\end{minted}

The monoid has set $G = \{\text{touched, untouched, does not terminate, terminates}\}$, and satisfies the monoid axioms as follows:
\begin{itemize}
    \item Clearly, this operation satisfies the \textit{closure} property of the monoid, as the result type is always in the monoidal group $G$ (as given by the faux function signature \mintinline{haskell}{tagPlus :: Tag -> Tag -> Tag}).
    \item Furthermore, it can be seen that the value of `touched' acts as the \textit{identity} element of the monoid: \mintinline{haskell}{Touched `tagPlus` x} = \mintinline{haskell}{x `tagPlus` Touched} = \mintinline{haskell}{x}.
    \item Finally, through observation, it is clear that this operation is also \textit{associative}.
\end{itemize}

% subsubsection termination_states (end)

% subsection verifier_traversal (end)

\subsection{Semantic Inference} % (fold)
\label{sub:semantic_inference}
In some cases, the \gls{dsl} designer would potentially desire to write productions for which they define no semantics. 
While it is not possible to infer semantics for such rules in the general case (as the syntax of a production can be arbitrarily complex), \gls{metaspec} aims to support the obvious use case through a simple inference rule. \\

% subsection semantic_inference (end)

\subsection{Basic Semantic Form Verification} % (fold)
\label{sub:basic_semantic_form_verification}
% All three criteria (evals, subterms, etc)
% State the theorem that this algorithm works off.
% Need to build ALL the core theory here. 
% Talk about evaluation ordering and criteria here (how are the sub-evals done, how are the evaluation rules done)?
% Talk about the restrictions placed on the form such that it can be verified
% Really place the emphasis here on HOW placing these restrictions enables the semantic verification. 

% subsection basic_semantic_form_verification (end)

\subsection{Guard Checking} % (fold)
\label{sub:guard_checking}
% Discuss both the original intent, some attempted algorithmic designs
% Reference back to the section ruling full validation as out of scope. 
% Explain the (very simple) reasoning behind the final algorithm. 

% subsection guard_checking (end)

\subsection{Verification of Other Semantic Forms} % (fold)
\label{sub:verification_of_other_semantic_forms}
% The basic user-defined semantics aren't the only kind that exist in metaspec
% Need to detail the algorithms for verification of these other semantic forms.

\subsubsection{Verification of Special-Syntax Rules} % (fold)
\label{ssub:verification_of_special_syntax_rules}

% subsubsection verification_of_special_syntax_rules (end)

\subsubsection{Verification of Environment Input Rules} % (fold)
\label{ssub:verification_of_environment_input_rules}

% subsubsection verification_of_environment_input_rules (end)

\subsubsection{Verification of Environment Access Rules} % (fold)
\label{ssub:verification_of_environment_access_rules}
% Default values on missing keys
% User-supplied defaults provided via SSR (mention this and discuss the disparity - due to not considering this entirely until later). 

% subsubsection verification_of_environment_access_rules (end)

% subsection verification_of_other_semantic_forms (end)

% section the_core_algorithms (end)

\section{Special Language Features} % (fold)
\label{sec:special_language_features}
% Talk about the design of each of the language features, and prove the required termination properties here.
% Talk about WHY things were thought of - the design process.

The special language features in \gls{absol} and \gls{metaspec} are best thought of as the `standard library' for \gls{dsl} implementation.
Each of these features aims to bring some important functionality to the toolchain, whether that be types for the semantics, non-terminals for parsing or special semantic functions to bring added flexibility. 
The main idea behind each of these features is to aid the \gls{dsl} designer in creating their language. \\

The design process for these features was mainly derived from observation of the most useful features of other programming languages.
For \gls{absol} to be able to design useful \glspl{dsl}, it had to be able to support most of these features. 
To this end, the design process was initiated by making a list of all of the useful features that were commonly found in programming languages used today.
This list was refined through consideration of which features would be possible to represent in the `safe' environment of \gls{metaspec}, and which would be most useful (where useful is defined in a fashion mainly involving intuition).
What resulted was a minimal set of features that would allow languages designed in \gls{metaspec} to be competent and contain functionality to actually make them useful to the \gls{dsl} implementers and users. 

\subsection{Feature --- \texttt{base}} % (fold)
\label{sub:feature_base}
% Needs some utility functions for assesing length of productions (e.g. number of items in a syntax list)
% Utility special syntax for environment gets (for use in normal syntactic rules) -> provision of default values to ensure semantics always work (how does this differ)
% Storing and retrieval of constant keys (a design issue with the grammar)
% Need to add these to the grammar and parser

% subsection feature_base (end)

\subsection{Feature --- \texttt{number}} % (fold)
\label{sub:feature_number}

% subsection feature_number (end)

\subsection{Feature --- \texttt{string}} % (fold)
\label{sub:feature_string}

% subsection feature_string (end)

\subsection{Feature --- \texttt{list}} % (fold)
\label{sub:feature_list}

% subsection feature_list (end)

\subsection{Feature --- \texttt{matrix}} % (fold)
\label{sub:feature_matrix}

% subsection feature_matrix (end)

\subsection{Feature --- \texttt{traverse}} % (fold)
\label{sub:feature_traverse}

% subsection feature_traverse (end)

\subsection{Feature --- \texttt{funcall}} % (fold)
\label{sub:feature_funcall}
% How are resultant DSL programs intended to be executed - does this impact the design of the metacompiler at all?
% During the design process some significant thought was given to... however, as it became apparent that code generation would not be taking place in the scope of this project...
% Discuss how this is used to enable execution of DSL programs. 
% Semantics for this are checked at DSL compilation time (do the called functions exist?, are they valid non-recursive calls?, noop calls if invalid?) 

% subsection feature_funcall (end)

\subsection{Discounted Language Features} % (fold)
\label{sub:discounted_language_features}
The features that actually went through proper design work for use in \gls{metaspec} were not the only ones considered. 
In addition to those outlined in the preceding sections, the following features were considered safe to implement for \gls{metaspec}, but were not considered crucial enough to its functionality.
Omitting these from the project scope was a useful method of constraining the number of features that were required to be implemented.
They are as follows:
\begin{itemize}
    \item \textbf{Associative Arrays:} The ability to represent associative-array container types and the operations on them in metaspec. 
    This would be useful for representing key-value mappings and any other kind of named dictionary structure.
    \item \textbf{Random Number Generation:} It could be useful to be able to generate random numbers.
    Such a feature would likely include probabilistic distributions to assist in mathematical modelling.
    \item \textbf{Maybe:} A `Maybe' type, similar to that in Haskell.
    This would aid in the representation of error conditions in functions.
    \item \textbf{Either:} Also similar to Haskell, this would allow the representation of alternates in result types, and hence also assist in the representation of error conditions.
    \item \textbf{State:} The provision of a user controlled state (and associated operations) that can be passed around functions and interacted with entirely separately from the semantic environment.
    \item \textbf{Special Numbers:} A feature providing additional number types such as complex numbers, financial numbers and arbitrary precision floating-point types.
    \item \textbf{Mathematical Operations:} Sets of additional mathematical operations defined for both the standard \mintinline{text}{number} and the above special numbers features. 
\end{itemize}

All of these features would provide the appropriate operations, types and non-terminal definitions to allow working with these language features as required. 

% subsection discounted_language_features (end)

% section special_language_features (end)

% chapter architecture_and_algorithms (end)
